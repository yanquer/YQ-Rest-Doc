=======================================
分布式锁
=======================================

.. 参考: `秒懂消息队列MQ，万字总结带你全面了解消息队列MQ <https://developer.aliyun.com/article/953777>`_

参考: `Redis 分布式锁的正确实现原理演化历程与 Redisson 实战总结 <https://developer.aliyun.com/article/1432395?spm=a2c6h.14164896.0.0.224e47c524hkTm&scm=20140722.S_community@@文章@@1432395._.ID_1432395-RL_分布式锁-LOC_search~UND~community~UND~item-OR_ser-V_3-P0_2>`_

分布式锁应该满足哪些特性

- 互斥：在任何给定时刻，只有一个客户端可以持有锁；
- 无死锁：任何时刻都有可能获得锁，即使获取锁的客户端崩溃；
- 容错：只要大多数 Redis的节点都已经启动，客户端就可以获取和释放锁。

如何设置?

普通排他锁
=======================================

利用 :doc:`/docs/数据库/redis/redis` 的特性:
使用 ``SETNX key value`` 命令是实现「互斥」特性

如获取一个订单锁::

  127.0.0.1:6379> setnx lock:order_1 1
  (integer) 1

重复获取就失败::

  127.0.0.1:6379> setnx lock:order_1 2
  (integer) 0

但是会存在一个问题, 会出现如果客户端异常, 锁无法释放,
就会有死锁.

这个时候可以设置超时, 如30秒过期::

  127.0.0.1:6379> expire lock:order_1 30
  (integer) 1

但是这样写有个问题, 因为是两个命令, 不能保证同时执行成功(原子性)

所有可以这样写::

  127.0.0.1:6379> set lock:order_1 1 NX EX 30
  OK

这样写还不够，我们还要防止不能释放不是自己加的锁, 比如是A设置的,
B给del了. 考虑以下场景:

1. A 获取锁成功并设置设置 30 秒超时；
2. A 因为一些原因导致执行很慢（网络问题、发生 FullGC……），过了 30 秒依然没执行完，但是锁过期「自动释放了」；
3. B 申请加锁成功；
4. A 执行完成，执行 DEL 释放锁指令，这个时候就把 B 的锁给释放了。

有个关键问题需要解决：自己的锁只能自己来释放

这个时候可以从value上入手, 比如设置其前缀为自己的标识,
**del前检查一下是不是自己的锁**

这个时候已经比较完美了, 一般都是如此使用

正确设置锁超时
=======================================

锁的超时时间怎么计算合适呢？
  这个时间不能瞎写，一般要根据在测试环境多次测试，然后压测多轮之后，比如计算出平均执行时间 200 ms。
  那么锁的超时时间就放大为平均执行时间的 3~5 倍。
为啥要放放大呢？
  因为如果锁的操作逻辑中有网络 IO 操作、JVM FullGC 等，线上的网络不会总一帆风顺，我们要给网络抖动留有缓冲时间。
那我设置更大一点，比如设置 1 小时不是更安全？
  不要钻牛角，多大算大？

  设置时间过长，一旦发生宕机重启，就意味着 1 小时内，分布式锁的服务全部节点不可用。
  你要让运维手动删除这个锁么？
  只要运维真的不会打你。
有没有完美的方案呢？不管时间怎么设置都不大合适。
  我们可以让获得锁的线程开启一个守护线程，用来给快要过期的锁「续航」。

  加锁的时候设置一个过期时间，同时客户端开启一个「守护线程」，定时去检测这个锁的失效时间。

  如果快要过期，但是业务逻辑还没执行完成，自动对这个锁进行续期，重新设置过期时间。
这个道理行得通，可我写不出。
  别慌，已经有一个库把这些工作都封装好了他叫 **Redisson**.
  在使用分布式锁时，它就采用了 **自动续期** 的方案来避免锁过期，这个守护线程我们一般也把它叫做 **看门狗** 线程。
一路优化下来，方案似乎比较「严谨」了，抽象出对应的模型如下
   1. 通过 SET lock_resource_name random_value NX EX expire_time，同时启动守护线程为快要过期但还没执行完的客户端的锁续命;
   2. 客户端执行业务逻辑操作共享资源；
   3. 通过脚本释放锁，先 get 判断锁是否是自己加的，再执行 DEL。

但是还是有以下问题

- 可重入锁如何实现？
- 主从架构崩溃恢复导致锁丢失如何解决？
- 客户端加锁的位置有门道么？

实现可重入锁
=======================================

当一个线程执行一段代码成功获取锁之后，继续执行时，
又遇到加锁的代码，可重入性就就保证线程能继续执行，
而不可重入就是需要等待锁释放之后，再次获取锁成功，才能继续往下执行。

Redis Hash 可重入锁
---------------------------------------

**Redisson** 类库就是通过 Redis Hash 来实现可重入锁

- 当线程拥有锁之后，往后再遇到加锁方法，直接将加锁次数加 1，然后再执行方法逻辑。
- 退出加锁方法之后，加锁次数再减 1，当加锁次数为 0 时，锁才被真正的释放。
- 可以看到可重入锁最大特性就是计数，计算加锁的次数。
- 所以当可重入锁需要在分布式环境实现时，我们也就需要统计加锁次数。

大致逻辑:

- 首先 Redis ``exists`` 命令判断当前 lock 这个锁是否存在
- 如果锁不存在的话，直接使用 ``hincrby`` 创建一个键为 lock hash 表，
  并且为 Hash 表中键为 uuid 初始化为 0，然后再次加 1，最后再设置过期时间
- 如果当前锁存在，则使用 ``hexists`` 判断当前 lock 对应的 hash 表中是否存在 uuid 这个键，
  如果存在，再次使用 hincrby 加 1，最后再次设置过期时间
- 最后如果上述两个逻辑都不符合，说明别人拿了锁.

主从架构带来的问题
=======================================

之前分析的场景都是，锁在「单个」Redis 实例中可能产生的问题，并没有涉及到 Redis 主从模式导致的问题。
我们通常使用「Cluster 集群」或者「哨兵集群」的模式部署保证高可用。

这两个模式都是基于「主从架构数据同步复制」实现的数据同步，而 Redis 的主从复制默认是异步的。

以下内容来自于官方文档 `Redis官方`_

我们试想下如下场景会发生什么问题：

1. 客户端 A 在 master 节点获取锁成功。
2. 还没有把获取锁的信息同步到 slave 的时候，master 宕机。
3. slave 被选举为新 master，这时候没有客户端 A 获取锁的数据。
4. 客户端 B 就能成功的获得客户端 A 持有的锁，违背了分布式锁定义的互斥。

虽然这个概率极低，但是我们必须得承认这个风险的存在。

❝Redis 的作者提出了一种解决方案，叫 Redlock（红锁）

Redis 的作者为了统一分布式锁的标准，搞了一个 Redlock，
算是 Redis 官方对于实现分布式锁的指导规范, `Redis官方`_ ,
但是这个 Redlock 也被国外的一些分布式专家给喷了。
因为它也不完美，有“漏洞”。

什么是 Redlock
---------------------------------------

可以看官方文档( `Redis官方`_ )，以下来自官方文档的翻译。

想用使用 Redlock，官方建议在不同机器上部署 5 个 Redis 主节点，
节点都是完全独立，也不使用主从复制，使用多个节点是为容错。
一个客户端要获取锁有 5 个步骤：

1. 客户端获取当前时间 T1（毫秒级别）；
2. 使用相同的 key和 value顺序尝试从 N个 Redis实例上获取锁。
3. 每个请求都设置一个超时时间（毫秒级别），该超时时间要远小于锁的有效时间，这样便于快速尝试与下一个实例发送请求。
   比如锁的自动释放时间 10s，则请求的超时时间可以设置 5~50 毫秒内，这样可以防止客户端长时间阻塞。
4. 客户端获取当前时间 T2 并减去步骤 1 的 T1 来计算出获取锁所用的时间（T3 = T2 -T1）。
   当且仅当客户端在大多数实例（N/2 + 1）获取成功，且获取锁所用的总时间 T3 小于锁的有效时间，才认为加锁成功，否则加锁失败。
5. 如果第 3 步加锁成功，则执行业务逻辑操作共享资源，
   key 的真正有效时间等于有效时间减去获取锁所使用的时间（步骤 3 计算的结果）。

   如果因为某些原因，获取锁失败（没有在至少 N/2+1 个 Redis 实例取到锁或者取锁时间已经超过了有效时间），
   客户端应该在所有的 Redis 实例上进行解锁（即便某些 Redis 实例根本就没有加锁成功）。

  另外部署实例的数量要求是奇数，为了能很好的满足过半原则，
  如果是 6 台则需要 4 台获取锁成功才能认为成功，所以奇数更合理

事情可没这么简单，Redis 作者把这个方案提出后，受到了业界著名的分布式系统专家的质疑。
两人好比神仙打架，两人一来一回论据充足的对一个问题提出很多论断……::

  • Martin Kleppmann 提出质疑的博客：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html
  • Redlock 设计者的回复：http://antirez.com/news/101

Redlock 是与非
---------------------------------------

Martin Kleppmann 认为锁定的目的是为了保护对共享资源的读写，而分布式锁应该「高效」和「正确」

- 高效性：分布式锁应该要满足高效的性能，Redlock 算法向 5 个节点执行获取锁的逻辑性能不高，成本增加，复杂度也高；
- 正确性：分布式锁应该防止并发进程在同一时刻只能有一个线程能对共享数据读写。

出于这两点，我们没必要承担 Redlock 的成本和复杂，运行 5 个 Redis 实例并判断加锁是否满足大多数才算成功。
主从架构崩溃恢复极小可能发生，这没什么大不了的。使用单机版就够了，Redlock 太重了，没必要。

Martin 认为 Redlock 根本达不到安全性的要求，也依旧存在锁失效的问题！

Martin 的结论

1. Redlock 不伦不类：对于偏好效率来讲，Redlock 比较重，没必要这么做，而对于偏好正确性来说，Redlock 是不够安全的。
2. 时钟假设不合理：该算法对系统时钟做出了危险的假设（假设多个节点机器时钟都是一致的），如果不满足这些假设，锁就会失效。
3. 无法保证正确性：Redlock 不能提供类似 fencing token 的方案，所以解决不了正确性的问题。为了正确性，请使用有「共识系统」的软件，例如 Zookeeper。

Redis 作者 Antirez 的反驳

在 Redis 作者的反驳文章中，有 3 个重点：

- 时钟问题：Redlock 并不需要完全一致的时钟，只需要大体一致就可以了，允许有「误差」，
  只要误差不要超过锁的租期即可，这种对于时钟的精度要求并不是很高，而且这也符合现实环境。
- 网络延迟、进程暂停问题：
- 客户端在拿到锁之前，无论经历什么耗时长问题，Redlock 都能够在第 3 步检测出来
- 客户端在拿到锁之后，发生 NPC，那 Redlock、Zookeeper 都无能为力
- 质疑 fencing token 机制。

Redisson 分布式锁
=======================================



.. _Redis官方: https://redis.io/topics/distlock


